from fastapi import FastAPI
import feedparser
from bs4 import BeautifulSoup
import requests
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, desc
from sqlalchemy.orm import sessionmaker, declarative_base
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="News Scraper API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================================================
# CONFIGURA√á√ÉO DO BANCO DE DADOS
# =========================================================
engine = create_engine("sqlite:///news.db", echo=False)
Base = declarative_base()

class News(Base):
    __tablename__ = "news"
    id = Column(Integer, primary_key=True)
    title = Column(String(255), unique=True)
    link = Column(String(500), unique=True)
    summary = Column(Text)
    image = Column(String(500))
    published = Column(DateTime, default=datetime.utcnow)

Base.metadata.create_all(bind=engine)
Session = sessionmaker(bind=engine)
session = Session()

# =========================================================
# FUN√á√ÉO AUXILIAR PARA PEGAR A PRIMEIRA IMAGEM
# =========================================================
def get_first_image(url):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        img_tag = soup.find("img")
        if img_tag and img_tag.get("src"):
            src = img_tag["src"]
            if src.startswith("//"):
                src = "https:" + src
            elif src.startswith("/"):
                domain = url.split("/")[2]
                src = f"https://{domain}{src}"
            return src
    except Exception:
        pass
    return None

# =========================================================
# FUN√á√ÉO PARA BUSCAR E SALVAR NOT√çCIAS
# =========================================================
def fetch_news():
    feeds = [
        "https://admin.cnnbrasil.com.br/feed/",
        "https://g1.globo.com/rss/g1/saude/",
        "https://feeds.bbci.co.uk/portuguese/topics/c404v09lmw9t/rss.xml",
        "https://rss.dw.com/rdf/rss-pt-saude",
    ]

    count_new = 0

    for url in feeds:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                title = entry.title
                link = entry.link.lower()  # üëà transforma em min√∫sculo pra facilitar
                summary = entry.get("summary", "")
                clean_summary = BeautifulSoup(summary, "html.parser").get_text()

                # ü©∫ FILTRO: s√≥ mant√©m se a URL indicar que √© not√≠cia de sa√∫de
                if not any(keyword in link for keyword in ["/saude", "health", "sa√∫de", "pt-saude"]):
                    continue  # pula not√≠cias fora da √°rea da sa√∫de

                # Verifica se j√° existe
                exists = session.query(News).filter(
                    (News.link == link) | (News.title == title)
                ).first()
                if exists:
                    continue

                # Pega imagem
                image_url = get_first_image(link)

                try:
                    news_item = News(
                        title=title,
                        link=link,
                        summary=clean_summary,
                        image=image_url,
                        published=datetime.utcnow()
                    )
                    session.add(news_item)
                    session.commit()
                    count_new += 1
                except Exception:
                    session.rollback()
        except Exception:
            continue

    return count_new



# =========================================================
# ROTAS
# =========================================================
@app.get("/atualizar")
def atualizar_noticias():
    novas = fetch_news()
    return {
        "message": f"‚úÖ {novas} novas not√≠cias de sa√∫de adicionadas." if novas > 0
        else "üü¢ As not√≠cias j√° est√£o atualizadas."
    }

@app.get("/noticias")
def listar_noticias():
    noticias = session.query(News).order_by(desc(News.published)).limit(30).all()
    return [
        {
            "id": n.id,
            "title": n.title,
            "link": n.link,
            "summary": n.summary,
            "image": n.image,
            "published": n.published.strftime("%Y-%m-%d")
        }
        for n in noticias
    ]

@app.get("/noticias/{id}")
def obter_noticia(id: int):
    noticia = session.query(News).filter_by(id=id).first()
    if not noticia:
        return {"error": "Not√≠cia n√£o encontrada"}

    return {
        "id": noticia.id,
        "title": noticia.title,
        "link": noticia.link,
        "summary": noticia.summary,
        "published": noticia.published.strftime("%Y-%m-%d"),
        "image": getattr(noticia, "image", None)
    }